{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b1fd46-487a-47b7-9c79-0b6d08d864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d5f5-f199-4b45-8aeb-b0975bb74760",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8fd0-c238-4a57-8e2b-98bf085aa948",
   "metadata": {},
   "source": [
    "# Pre-Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences. the sentences are the stream of words in single line\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(\\.|\\n|\\t)')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "def preProcess_chunk_text(text: str, chunk_size=10) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contains sentences, and the number of words is minimum the chunk_size.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size (int): Maximum number of words per chunk (Default: 20).\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of chunks, where each chunk contains sentences with a word count greater than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = segment_into_sentences(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if(sentence == \".\"):\n",
    "            continue\n",
    "        chunk.append(sentence)\n",
    "        word_count += len(sentence.split())\n",
    "        if(word_count>chunk_size):\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "            \n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c610b-d04e-4a07-8d6b-31e7508b2f72",
   "metadata": {},
   "source": [
    "# Form Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ec7d82-2c7c-4194-ba0c-9def823b9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"BERT_FineTuned_Model2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8dcb2ac-13ce-4a22-b42f-55449db30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text: str):\n",
    "    \"\"\"\n",
    "    extracts the key phases and noun from the question.\n",
    "    Parametes:\n",
    "        text(str): text form which keywords will be extracted\n",
    "    Returns: \n",
    "        List[str]: a list of key phrases.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ']]\n",
    "    return keywords\n",
    "    \n",
    "def get_sentence_embedding(text:str):\n",
    "    \"\"\"\n",
    "    Convert a sentence into its dense embedding usng RoBERTa.\n",
    "    Parameters:\n",
    "        text(str): the text to be converted into embedding.\n",
    "    Returns\n",
    "        embedding: the dense vector representing embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = hidden_states.mean(dim = 1).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentences(question: str, resume_chunks: list[str], top_k = 7)->list[str]:\n",
    "    \"\"\"\n",
    "    find the most relevant sentences in the resume for a given question\n",
    "\n",
    "    Parameters: \n",
    "        questions(str): the user's query.\n",
    "        resume_chunks(list): list of resume text chunks\n",
    "        top_k: maximum number of relvant sentences required(default 7)\n",
    "    Returns:\n",
    "        List[str]: the list of most relevant chunks\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    question_embedding = get_sentence_embedding(question).numpy()\n",
    "    chunk_embeddings = [get_sentence_embedding(chunk).numpy() for chunk in resume_chunks]\n",
    "\n",
    "    chunk_embeddings = [embedding.flatten() for embedding in chunk_embeddings]\n",
    "    similarities = cosine_similarity([question_embedding], chunk_embeddings)\n",
    "\n",
    "    keyword_boost = []\n",
    "    for chunk, sim in zip(resume_chunks, similarities[0]):\n",
    "        chunk_keywords = extract_keywords(chunk)\n",
    "        match_count = len([chunk_keyword for chunk_keyword in chunk_keywords if chunk_keyword in keywords])\n",
    "        boosted_sim = sim + match_count*0.54\n",
    "        keyword_boost.append((boosted_sim, chunk))\n",
    "    keyword_boost.sort(key = lambda x:x[0], reverse = True)\n",
    "    top_k_chunks = [chunk for _, chunk in keyword_boost[:top_k]]\n",
    "\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c9122-3699-4a79-95bc-37965de3e773",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc10b78f-a4e9-4558-9073-5e103f30a162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e8991a7ede484b9cce9018185787b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9adcde9c55456fa2103815bd1bef26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c474c8b741141e6a4e4df86001ca2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaa8304793d4463b8d277c329b50e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32f5d84d1a148489492183d408892d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qa_model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_tokenizer = BertTokenizer.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1ec3432-8154-481c-9cdb-d44d44b1d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, relevant_sentences:list[str])->str:\n",
    "    \"\"\"\n",
    "    Generate a clear and concise answer to the question based on the most relevant sentences.\n",
    "\n",
    "    Parameters:\n",
    "        question(str):The user's query.\n",
    "        relevant_sentences(list[str]): list of most relevant sentences.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generate answer.\n",
    "    \"\"\"\n",
    "\n",
    "    context = \" \".join(relevant_sentences)\n",
    "    inputs = qa_tokenizer.encode_plus(question, context, return_tensors='pt', truncation = True, max_length = 512)\n",
    "    # print(inputs)\n",
    "    with torch.no_grad():\n",
    "        output = qa_model(**inputs)\n",
    "        start_scores,end_scores, = output.start_logits, output.end_logits\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index]))\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db028614-d2fd-4f77-b787-e78346cdfa24",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'problem solving , leadership , collaborative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = extract_text(\"D:\\Resumes\\off_campus.pdf\")\n",
    "resume_chunks = preProcess_chunk_text(text) \n",
    "question = \"what are my soft skills.\"\n",
    "top_relevant_sentences = get_most_relevant_sentences(question, resume_chunks, 10)\n",
    "answer = generate_answer(question, top_relevant_sentences)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcf719-5cf6-4847-bf0b-eed02052369c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
