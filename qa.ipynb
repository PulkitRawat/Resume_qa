{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320ff25b-7ce9-4b10-ae98-9f6cec720d43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromaDB\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromaDB)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (2.10.6)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromaDB)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl.metadata (262 bytes)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.110.3)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromaDB)\n",
      "  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.15.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.31.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromaDB)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromaDB)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromaDB)\n",
      "  Downloading opentelemetry_sdk-1.31.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromaDB)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 30.7/67.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromaDB)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromaDB)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromaDB)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromaDB)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromaDB)\n",
      "  Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.9/42.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (13.3.5)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromaDB) (23.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromaDB)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromaDB) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromaDB) (0.37.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromaDB) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.28.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (0.58.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (1.26.18)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (4.25.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromaDB) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromaDB) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB) (1.63.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromaDB)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB) (3.8.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromaDB)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromaDB) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromaDB) (1.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromaDB) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromaDB) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromaDB) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromaDB) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromaDB) (0.27.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromaDB) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromaDB) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (4.7.2)\n",
      "INFO: pip is looking at multiple versions of googleapis-common-protos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromaDB) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromaDB) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromaDB) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromaDB) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromaDB) (2.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromaDB) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromaDB) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromaDB) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (0.4.8)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "   ---------------------------------------- 0.0/611.1 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 194.6/611.1 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 430.1/611.1 kB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 611.1/611.1 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.9/151.9 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.8/152.8 kB ? eta 0:00:00\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.7/2.0 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/2.0 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.0-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.9/55.9 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.52b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.4/183.4 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.0-py3-none-any.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.9/118.9 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.6/133.6 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.6/79.6 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.3/62.3 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 293.2/293.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 88.6/88.6 kB 5.2 MB/s eta 0:00:00\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 358.4/434.5 kB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.5/434.5 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading watchfiles-1.0.4-cp311-cp311-win_amd64.whl (284 kB)\n",
      "   ---------------------------------------- 0.0/284.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 284.2/284.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "   ---------------------------------------- 0.0/176.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 176.8/176.8 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53915 sha256=0d380d67cffa2e54ad8d067609440f3f30c7e1027cfbf13dad03bc932da03d10\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, websockets, tenacity, pyproject_hooks, protobuf, orjson, opentelemetry-util-http, oauthlib, mmh3, httptools, chroma-hnswlib, bcrypt, watchfiles, uvicorn, requests-oauthlib, posthog, opentelemetry-proto, googleapis-common-protos, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromaDB\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.6\n",
      "    Uninstalling protobuf-4.25.6:\n",
      "      Successfully uninstalled protobuf-4.25.6\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.63.0\n",
      "    Uninstalling googleapis-common-protos-1.63.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.63.0\n",
      "Successfully installed bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromaDB-0.6.3 durationpy-0.9 googleapis-common-protos-1.69.2 httptools-0.6.4 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 oauthlib-3.2.2 opentelemetry-exporter-otlp-proto-common-1.31.0 opentelemetry-exporter-otlp-proto-grpc-1.31.0 opentelemetry-instrumentation-0.52b0 opentelemetry-instrumentation-asgi-0.52b0 opentelemetry-instrumentation-fastapi-0.52b0 opentelemetry-proto-1.31.0 opentelemetry-sdk-1.31.0 opentelemetry-semantic-conventions-0.52b0 opentelemetry-util-http-0.52b0 orjson-3.10.15 posthog-3.21.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 tenacity-9.0.0 uvicorn-0.34.0 watchfiles-1.0.4 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\~crypt'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 2.17.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.33.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.33.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b1fd46-487a-47b7-9c79-0b6d08d864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "chroma_client = chromadb.PersistentClient(path = \"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name='resume_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d5f5-f199-4b45-8aeb-b0975bb74760",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8fd0-c238-4a57-8e2b-98bf085aa948",
   "metadata": {},
   "source": [
    "# Pre-Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences. the sentences are the stream of words in single line\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(\\.|\\n|\\t)')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "def preProcess_chunk_text(text: str, chunk_size=10) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contains sentences, and the number of words is minimum the chunk_size.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size (int): Maximum number of words per chunk (Default: 20).\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of chunks, where each chunk contains sentences with a word count greater than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = segment_into_sentences(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if(sentence == \".\"):\n",
    "            continue\n",
    "        chunk.append(sentence)\n",
    "        word_count += len(sentence.split())\n",
    "        if(word_count>chunk_size):\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "            \n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c610b-d04e-4a07-8d6b-31e7508b2f72",
   "metadata": {},
   "source": [
    "# Form Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ec7d82-2c7c-4194-ba0c-9def823b9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"BERT_FineTuned_Model2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8dcb2ac-13ce-4a22-b42f-55449db30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text: str):\n",
    "    \"\"\"\n",
    "    extracts the key phrases  from the question.\n",
    "    Parametes:\n",
    "        text(str): text from which keywords will be extracted\n",
    "    Returns: \n",
    "        List[str]: a list of key words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ','VERB']] \n",
    "    return keywords\n",
    "    \n",
    "def get_sentence_embedding(text:str):\n",
    "    \"\"\"\n",
    "    Convert a sentence into its dense embedding usng RoBERTa.\n",
    "    Parameters:\n",
    "        text(str): the text to be converted into embedding.\n",
    "    Returns\n",
    "        embedding: the dense vector representing embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = hidden_states.mean(dim = 1).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e6629ca-b886-42fe-87d8-aa1c92f7bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(text_chunks:list[str]):\n",
    "    \"\"\"\n",
    "    stores text chunks and their embeddings into chroma DB.\n",
    "\n",
    "    Parameters:\n",
    "        text_chunks(list[str]): list of extracted text chunks.\n",
    "    Returns:\n",
    "        None(Stores the embedding in chromaDB)\n",
    "    \"\"\"\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        embedding = get_sentence_embedding(chunk).tolist()\n",
    "    \n",
    "        collection.add(\n",
    "            ids = [str(uuid.uuid4())],\n",
    "            embeddings = [embedding],\n",
    "            metadatas = [{'text':chunk}]\n",
    "        )\n",
    "    print(\"text_chunk was stored as embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1c7ec-ca0b-4824-835b-d2c6db1d662d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentences(question: str, top_k = 7)->list[str]:\n",
    "    \"\"\"\n",
    "    find the most relevant sentences in the resume for a given question using chromaDB and boost the relevance using keyword matching\n",
    "\n",
    "    Parameters: \n",
    "        questions(str): the user's query.\n",
    "        resume_chunks(list): list of resume text chunks\n",
    "        top_k: maximum number of relvant sentences required(default 7)\n",
    "    Returns:\n",
    "        List[str]: the list of most relevant chunks\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    question_embedding = get_sentence_embedding(question).numpy()\n",
    "    results = collection.query(\n",
    "        query_embeddings = [question_embedding],\n",
    "        n_results = int(top_k*5)\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = results['metadatas'][0]\n",
    "    distances = results['distances'][0]\n",
    "\n",
    "    text_chunks = [chunk['text'] for chunk in retrieved_chunks]\n",
    "    keyword_boost = []\n",
    "    for chunk,distance in zip(text_chunks, distances):\n",
    "    \n",
    "        chunk_keywords = extract_keywords(chunk)\n",
    "\n",
    "        match_count = len([chunk_keyword for chunk_keyword in chunk_keywords if chunk_keyword in keywords])\n",
    "        print(chunk)\n",
    "        print(chunk_keywords)\n",
    "        print(match_count)\n",
    "        \n",
    "        sim = 1/(1+distance)\n",
    "        boosted_score= sim + match_count*0.54\n",
    "        keyword_boost.append((boosted_score, chunk))\n",
    "        \n",
    "    keyword_boost.sort(key = lambda x:x[0], reverse = True)\n",
    "    \n",
    "    top_k_chunks = [chunk for _, chunk in keyword_boost[:top_k]]\n",
    "\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c9122-3699-4a79-95bc-37965de3e773",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc10b78f-a4e9-4558-9073-5e103f30a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qa_model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_tokenizer = BertTokenizer.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1ec3432-8154-481c-9cdb-d44d44b1d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, relevant_sentences:list[str])->str:\n",
    "    \"\"\"\n",
    "    Generate a clear and concise answer to the question based on the most relevant sentences.\n",
    "\n",
    "    Parameters:\n",
    "        question(str):The user's query.\n",
    "        relevant_sentences(list[str]): list of most relevant sentences.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generate answer.\n",
    "    \"\"\"\n",
    "\n",
    "    context = \" \".join(relevant_sentences)\n",
    "    inputs = qa_tokenizer.encode_plus(question, context, return_tensors='pt', truncation = True, max_length = 512)\n",
    "    # print(inputs)\n",
    "    with torch.no_grad():\n",
    "        output = qa_model(**inputs)\n",
    "        start_scores,end_scores, = output.start_logits, output.end_logits\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index]))\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db028614-d2fd-4f77-b787-e78346cdfa24",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2a7c88c-3df2-4890-8202-bc6768b546ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skills']\n"
     ]
    }
   ],
   "source": [
    "print(extract_keywords(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collection deleted and recreated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 35 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_chunk was stored as embedding\n",
      "Retrieved Chunks:\n",
      "1. Enhanced response relevance by identifying question keywords, prioritizing matching text chunks, and applying cosine similarity for optimal answer retrieval (Distance: 206.3634863487476)\n",
      "2. Improved financial decision making by leveraging GPT to generate comparisons, highlighting advantages of suggested funds over the users current selections (Distance: 222.13381655260784)\n",
      "3. Ranked investment funds by assigning feature based weights on metrics like rolling returns, alpha, beta to optimize fund selection based on users risk profile (Distance: 254.72090680731392)\n",
      "4. Fund Suggestion Model  Developed a fund recommendation system that analyzed a users existing portfolio and suggested better funds within the same risk profile, category and AUM (Distance: 267.50216126960544)\n",
      "5. Optimized chatbot performance by integrating RAG, Supplying top relevant QnA pairs as context and prompting GPT to generate clear and concise responses to user queries (Distance: 272.22540042854956)\n",
      "6. Estimated vehicle speed and MTTZ by tracking trajectory with Supervision and incorporating user reaction time for real-time pedestrian alerts (Distance: 274.8710391077491)\n",
      "7. Built an interactive web interface using Flask, enabling users to upload resumes and receive precise, context-aware responses to their queries (Distance: 281.41875127388556)\n",
      "8. Projects Enhancement of Haptic Alert System May 2024 - July 2024 IIT Roorkee  Enhanced vehicle detection accuracy by fine-tuning YOLO for long-range detection in Indian road conditions enabling identification over a greater distance (Distance: 288.08074999332723)\n",
      "9. Modeled stock price movements by training Random Forest, XGBoost, and LSTM on sentiment, topic and embeddings uncovering trends between market sentiment and price shifts (Distance: 290.54588230250346)\n",
      "10. Enhanced response accuracy by generating embedding of QnA pairs and implementing a vector similarity based retrieval system to fetch the most relevant solutions (Distance: 290.8059317916297)\n",
      "11. Automated Resume Information Extraction August 2024- Oct 2024 Self Project  Developed an AI-driven Q&A system by extracting text from PDFs/images, chunking data, and generating embeddings using a fine-tuned BERT base-uncased model (Distance: 318.1907042056548)\n",
      "12. Developed a smart haptic system by integrating MQTT based communication between Raspberry Pi and Arduino, enabling vibration intensity proportional to vehicle proximity (Distance: 320.10281548263214)\n",
      "13. Stock Sentiment Analysis Jan 2025- Feb 2025 Self Project  Extracted and processed financial news data by scraping headlines, applying NLP techniques for tokenization, and computing sentiment scores using VADER (Distance: 329.95600375878996)\n",
      "14. Additional Courses: DeepLearning ai course for Deep Learning and its optimization, Analytics Managing and Decision Making (Distance: 340.991616682572)\n",
      "15. Engineered predictive features by generating 20-dimensional word2vec embedding and introducing topics via fine-tuned BERTopic for enhanced stock movement correlation (Distance: 354.9011292899601)\n",
      "16. com/PulkitRawat Experience BuildWealth Technologies Private Limited Remote DATA SCIENCE INTERN December 2023-April 2024 Customer Support QnA  Automated customer query resolution by extracting structured QnA pairs from freshdesk support tickets using GPT-based transformation, storing them efficiently in CSV format (Distance: 356.40557200601376)\n",
      "17. Education Indian Institute of Technology, Roorkee Roorkee, India B TECH 2021-2025 Additional Information Technical Skills: C++, Python, SQL, ML, NLP, transformers, LLMs, Data Analysis (Distance: 359.55132499211817)\n",
      "18. Pulkit Rawat 6261360983 | rpulkit610@gmail com | https://www linkedin com/in/pulkit-rawat | https://github (Distance: 443.1846563071741)\n",
      "Enhanced response relevance by identifying question keywords, prioritizing matching text chunks, and applying cosine similarity for optimal answer retrieval\n",
      "['enhanced', 'response', 'relevance', 'identifying', 'question', 'keywords', 'prioritizing', 'matching', 'text', 'chunks', 'applying', 'cosine', 'similarity', 'optimal', 'answer', 'retrieval']\n",
      "0\n",
      "Improved financial decision making by leveraging GPT to generate comparisons, highlighting advantages of suggested funds over the users current selections\n",
      "['improved', 'financial', 'decision', 'making', 'leveraging', 'gpt', 'generate', 'comparisons', 'highlighting', 'advantages', 'suggested', 'funds', 'users', 'current', 'selections']\n",
      "0\n",
      "Ranked investment funds by assigning feature based weights on metrics like rolling returns, alpha, beta to optimize fund selection based on users risk profile\n",
      "['ranked', 'investment', 'funds', 'assigning', 'feature', 'based', 'weights', 'metrics', 'rolling', 'returns', 'alpha', 'beta', 'optimize', 'fund', 'selection', 'based', 'users', 'risk', 'profile']\n",
      "0\n",
      "Fund Suggestion Model  Developed a fund recommendation system that analyzed a users existing portfolio and suggested better funds within the same risk profile, category and AUM\n",
      "['fund', 'suggestion', 'model', 'developed', 'fund', 'recommendation', 'system', 'analyzed', 'users', 'existing', 'portfolio', 'suggested', 'better', 'funds', 'same', 'risk', 'profile', 'category', 'aum']\n",
      "0\n",
      "Optimized chatbot performance by integrating RAG, Supplying top relevant QnA pairs as context and prompting GPT to generate clear and concise responses to user queries\n",
      "['optimized', 'chatbot', 'performance', 'integrating', 'rag', 'supplying', 'top', 'relevant', 'qna', 'pairs', 'context', 'prompting', 'gpt', 'generate', 'clear', 'concise', 'responses', 'user', 'queries']\n",
      "0\n",
      "Estimated vehicle speed and MTTZ by tracking trajectory with Supervision and incorporating user reaction time for real-time pedestrian alerts\n",
      "['estimated', 'vehicle', 'speed', 'mttz', 'tracking', 'trajectory', 'supervision', 'incorporating', 'user', 'reaction', 'time', 'real', 'time', 'pedestrian', 'alerts']\n",
      "0\n",
      "Built an interactive web interface using Flask, enabling users to upload resumes and receive precise, context-aware responses to their queries\n",
      "['built', 'interactive', 'web', 'interface', 'using', 'flask', 'enabling', 'users', 'upload', 'resumes', 'receive', 'precise', 'context', 'aware', 'responses', 'queries']\n",
      "0\n",
      "Projects Enhancement of Haptic Alert System May 2024 - July 2024 IIT Roorkee  Enhanced vehicle detection accuracy by fine-tuning YOLO for long-range detection in Indian road conditions enabling identification over a greater distance\n",
      "['projects', 'enhancement', 'haptic', 'alert', 'system', 'may', 'july', 'iit', 'roorkee', 'enhanced', 'vehicle', 'detection', 'accuracy', 'tuning', 'yolo', 'long', 'range', 'detection', 'indian', 'road', 'conditions', 'enabling', 'identification', 'greater', 'distance']\n",
      "0\n",
      "Modeled stock price movements by training Random Forest, XGBoost, and LSTM on sentiment, topic and embeddings uncovering trends between market sentiment and price shifts\n",
      "['modeled', 'stock', 'price', 'movements', 'training', 'random', 'forest', 'xgboost', 'lstm', 'sentiment', 'topic', 'embeddings', 'uncovering', 'trends', 'market', 'sentiment', 'price', 'shifts']\n",
      "0\n",
      "Enhanced response accuracy by generating embedding of QnA pairs and implementing a vector similarity based retrieval system to fetch the most relevant solutions\n",
      "['enhanced', 'response', 'accuracy', 'generating', 'embedding', 'qna', 'pairs', 'implementing', 'vector', 'similarity', 'based', 'retrieval', 'system', 'fetch', 'relevant', 'solutions']\n",
      "0\n",
      "Automated Resume Information Extraction August 2024- Oct 2024 Self Project  Developed an AI-driven Q&A system by extracting text from PDFs/images, chunking data, and generating embeddings using a fine-tuned BERT base-uncased model\n",
      "['automated', 'resume', 'information', 'extraction', 'august', '2024-', 'oct', 'self', 'project', 'developed', 'ai', 'driven', 'q&a', 'system', 'extracting', 'text', 'pdfs', 'images', 'chunking', 'data', 'generating', 'embeddings', 'using', 'tuned', 'bert', 'base', 'uncased', 'model']\n",
      "0\n",
      "Developed a smart haptic system by integrating MQTT based communication between Raspberry Pi and Arduino, enabling vibration intensity proportional to vehicle proximity\n",
      "['developed', 'smart', 'haptic', 'system', 'integrating', 'mqtt', 'based', 'communication', 'raspberry', 'pi', 'arduino', 'enabling', 'vibration', 'intensity', 'proportional', 'vehicle', 'proximity']\n",
      "0\n",
      "Stock Sentiment Analysis Jan 2025- Feb 2025 Self Project  Extracted and processed financial news data by scraping headlines, applying NLP techniques for tokenization, and computing sentiment scores using VADER\n",
      "['stock', 'sentiment', 'analysis', 'jan', 'feb', 'self', 'project', 'extracted', 'processed', 'financial', 'news', 'data', 'scraping', 'headlines', 'applying', 'nlp', 'techniques', 'tokenization', 'computing', 'sentiment', 'scores', 'using', 'vader']\n",
      "0\n",
      "Additional Courses: DeepLearning ai course for Deep Learning and its optimization, Analytics Managing and Decision Making\n",
      "['additional', 'courses', 'deeplearning', 'ai', 'course', 'deep', 'learning', 'optimization', 'analytics', 'managing', 'decision', 'making']\n",
      "0\n",
      "Engineered predictive features by generating 20-dimensional word2vec embedding and introducing topics via fine-tuned BERTopic for enhanced stock movement correlation\n",
      "['engineered', 'predictive', 'features', 'generating', 'dimensional', 'word2vec', 'embedding', 'introducing', 'topics', 'tuned', 'bertopic', 'enhanced', 'stock', 'movement', 'correlation']\n",
      "0\n",
      "com/PulkitRawat Experience BuildWealth Technologies Private Limited Remote DATA SCIENCE INTERN December 2023-April 2024 Customer Support QnA  Automated customer query resolution by extracting structured QnA pairs from freshdesk support tickets using GPT-based transformation, storing them efficiently in CSV format\n",
      "['com', 'pulkitrawat', 'experience', 'buildwealth', 'technologies', 'private', 'limited', 'remote', 'data', 'science', 'intern', 'december', 'april', 'customer', 'support', 'qna', 'automated', 'customer', 'query', 'resolution', 'extracting', 'structured', 'qna', 'pairs', 'freshdesk', 'support', 'tickets', 'using', 'gpt', 'based', 'transformation', 'storing', 'csv', 'format']\n",
      "0\n",
      "Education Indian Institute of Technology, Roorkee Roorkee, India B TECH 2021-2025 Additional Information Technical Skills: C++, Python, SQL, ML, NLP, transformers, LLMs, Data Analysis\n",
      "['education', 'indian', 'institute', 'technology', 'roorkee', 'roorkee', 'india', 'b', 'tech', 'additional', 'information', 'technical', 'skills', 'c++', 'python', 'sql', 'ml', 'nlp', 'transformers', 'llms', 'data', 'analysis']\n",
      "1\n",
      "Pulkit Rawat 6261360983 | rpulkit610@gmail com | https://www linkedin com/in/pulkit-rawat | https://github\n",
      "['pulkit', 'rawat', '|', 'rpulkit610@gmail', 'com', '|', 'https://www', 'linkedin', 'com', 'pulkit', 'rawat', '|', 'https://github']\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c + + , python , sql , ml , nlp , transformers , llms , data analysis'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = extract_text(\"D:\\Resumes\\offcampus\\Resume.pdf\")\n",
    "resume_chunks = preProcess_chunk_text(text) \n",
    "chroma_client.delete_collection(name=\"resume_embeddings\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"resume_embeddings\")  # Recreate it\n",
    "print(\"✅ Collection deleted and recreated!\")\n",
    "store_embeddings(resume_chunks)\n",
    "question = \"what are the skills?\"\n",
    "top_relevant_sentences = get_most_relevant_sentences(question)\n",
    "answer = generate_answer(question, top_relevant_sentences)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d47b3a-2acc-425a-a9e3-f66d25bba959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
