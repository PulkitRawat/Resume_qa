{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b1fd46-487a-47b7-9c79-0b6d08d864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d5f5-f199-4b45-8aeb-b0975bb74760",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8fd0-c238-4a57-8e2b-98bf085aa948",
   "metadata": {},
   "source": [
    "# Pre-Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(\\.|\\n|\\t)')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "def preProcess_chunk_text(text: str, chunk_size=10) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contains sentences, and the number of words in each chunk does not exceed \n",
    "    the specified chunk_size. Additionally, chunks are divided at line breaks (\\n) to ensure logical chunking.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size (int): Maximum number of words per chunk (Default: 20).\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of chunks, where each chunk contains sentences with a word count less than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = segment_into_sentences(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if(sentence == \".\"):\n",
    "            continue\n",
    "        chunk.append(sentence)\n",
    "        word_count += len(sentence.split())\n",
    "        if(word_count>chunk_size):\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "            \n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c610b-d04e-4a07-8d6b-31e7508b2f72",
   "metadata": {},
   "source": [
    "# Form Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20ec7d82-2c7c-4194-ba0c-9def823b9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"BERT_FineTuned_Model2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d8dcb2ac-13ce-4a22-b42f-55449db30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text: str):\n",
    "    \"\"\"\n",
    "    extracts the key phases and noun from the question.\n",
    "    Parametes:\n",
    "        text(str): text form which keywords will be extracted\n",
    "    Returns: \n",
    "        List[str]: a list of key phrases.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ']]\n",
    "    return keywords\n",
    "    \n",
    "def get_sentence_embedding(text:str):\n",
    "    \"\"\"\n",
    "    Convert a sentence into its dense embedding usng RoBERTa.\n",
    "    Parameters:\n",
    "        text(str): the text to be converted into embedding.\n",
    "    Returns\n",
    "        embedding: the dense vector representing embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = hidden_states.mean(dim = 1).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentences(question: str, resume_chunks: list[str], top_k = 7)->list[str]:\n",
    "    \"\"\"\n",
    "    find the most relevant sentences in the resume for a given question\n",
    "\n",
    "    Parameters: \n",
    "        questions(str): the user's query.\n",
    "        resume_chunks(list): list of resume text chunks\n",
    "        top_k: maximum number of relvant sentences required(default 7)\n",
    "    Returns:\n",
    "        List[str]: the list of most relevant chunks\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    question_embedding = get_sentence_embedding(question).numpy()\n",
    "    chunk_embeddings = [get_sentence_embedding(chunk).numpy() for chunk in resume_chunks]\n",
    "\n",
    "    chunk_embeddings = [embedding.flatten() for embedding in chunk_embeddings]\n",
    "    similarities = cosine_similarity([question_embedding], chunk_embeddings)\n",
    "\n",
    "    keyword_boost = []\n",
    "    for chunk, sim in zip(resume_chunks, similarities[0]):\n",
    "        chunk_keywords = extract_keywords(chunk)\n",
    "        match_count = len([chunk_keyword for chunk_keyword in chunk_keywords if chunk_keyword in keywords])\n",
    "        boosted_sim = sim + match_count*0.54\n",
    "        keyword_boost.append((boosted_sim, chunk))\n",
    "    keyword_boost.sort(key = lambda x:x[0], reverse = True)\n",
    "    top_k_chunks = [chunk for _, chunk in keyword_boost[:top_k]]\n",
    "\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db028614-d2fd-4f77-b787-e78346cdfa24",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDUCATION B Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning',\n",
       " 'Recommended targeted corrective actions to mitigate consumption spikes & optimize consumption efficiency',\n",
       " 'Utilized the GPT model to generate clear & concise response comparing the suggested with prior selections',\n",
       " 'Optimized response generation by assigning weights to sentences based on their alignment with the question',\n",
       " 'Extra curriculars: Head Research Analyst | Appetizer, Executive Member | STC',\n",
       " 'Enhanced BERT model, fine-tuned on a dataset of QnA pairs; generating response with weighted relevance',\n",
       " 'Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model',\n",
       " 'Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors',\n",
       " 'Developed anomaly detection techniques to accurately identify irregular energy consumption patterns',\n",
       " 'PDF Answering AI Apr 2023 - May 2023 Streamlined QnA by using BERT embeddings to retrieve question relevant sentences from PDF documents']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = extract_text(\"D:\\Resumes\\off_campus.pdf\")\n",
    "resume_chunks = preProcess_chunk_text(text) \n",
    "question = \"what are my soft skills.\"\n",
    "top_relevant_chunks = get_most_relevant_sentences(question, resume_chunks, 10)\n",
    "top_relevant_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
