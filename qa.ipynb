{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b1fd46-487a-47b7-9c79-0b6d08d864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d5f5-f199-4b45-8aeb-b0975bb74760",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8fd0-c238-4a57-8e2b-98bf085aa948",
   "metadata": {},
   "source": [
    "# Pre-Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "def preProcess_chunk_text(text:str, chunk_size = 20)->list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contain sentences, and number of words in each chunk does not exceed \n",
    "    specified chunk_size\n",
    "    Paramters:\n",
    "        text(str): parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size(int): Maximum number of words per chunk (Default: 20).\n",
    "    Returns:\n",
    "        list[list[str]]: list of chunks, where each chunk contains sentences having number of words is less than or equal to chunk_size.|\n",
    "        \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences= segment_into_sentences(text)\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    for sentence in sentences: \n",
    "        words = sentence.split()\n",
    "        word_count+= len(words)\n",
    "        if word_count>chunk_size:\n",
    "            word_count = len(words)\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "        chunk.append(sentence)\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c610b-d04e-4a07-8d6b-31e7508b2f72",
   "metadata": {},
   "source": [
    "# Form Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcb2ac-13ce-4a22-b42f-55449db30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text: str):\n",
    "    \"\"\"\n",
    "    extracts the key phases and noun from the question.\n",
    "    Parametes:\n",
    "        text(str): text form which keywords will be extracted\n",
    "    Returns: \n",
    "        List[str]: a list of key phrases.\n",
    "    \"\"\"\n",
    "    doc = nlp(question)\n",
    "    keywords = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ']]\n",
    "    return keywords\n",
    "    \n",
    "def get_sentence_embedding(text:str):\n",
    "    \"\"\"\n",
    "    Convert a sentence into its dense embedding usng RoBERTa.\n",
    "    Parameters:\n",
    "        text(str): the text to be converted into embedding.\n",
    "    Returns\n",
    "        embedding: the dense vector representing embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = hidden_states.mean(dim = 1).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentences(question: str, resume_chunks: list[str], top_k = 7)->list[str]:\n",
    "    \"\"\"\n",
    "    find the most relevant sentences in the resume for a given question\n",
    "\n",
    "    Parameters: \n",
    "        questions(str): the user's query.\n",
    "        resume_chunks(list): list of resume text chunks\n",
    "        top_k: maximum number of relvant sentences required(default 7)\n",
    "    Returns:\n",
    "        List[str]: the list of most relevant chunks\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    question_embedding = get_sentence_embedding(question)\n",
    "    chunk_embeddings = [get_sentence_embedding(chunk) for chunk in resume_chunks]\n",
    "\n",
    "    similarities = cosine_similarity([question_embedding.numpy()], [chunks.numpy() for chunks in chunk_embeddings])\n",
    "\n",
    "    keyword_boost = []\n",
    "    for chunk, sim in zip(resume_chunks, similarities[0]):\n",
    "        chunk_keywords = extract_keywords(chunk)\n",
    "        match_count = len(set(keywords) & set(chunk_keywords))\n",
    "        boosted_sim = sim + match_count*0.1\n",
    "        keyword_boost.append((boosted_sim, chunk))\n",
    "\n",
    "    keyword_boost.sort(key = lambda x:x[0], reverse = True)\n",
    "    top_k_chunks = [chunk for _, chunk in keyword_boost[:top_k]]\n",
    "\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db028614-d2fd-4f77-b787-e78346cdfa24",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f52f98a3-537d-4c70-a923-2d765b0d9378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PULKIT RAWAT UG (IV Year I Semester) Email: rpulkit610@gmail.com B.Tech.',\n",
       " '(Civil Engineering) p_rawat@ce.iitr.ac.in Contact No: 6261360983 linkedin.com/in/pulkit-rawat/ AREAS OF INTEREST Data Analytics, Data Science, Natural Language Processing, Data Structure and Algorithm, Gymnasium WORK EXPERIENCE Data Scientist , BuildWealth Technologies Pvt Ltd Dec 2023 - Apr 2023 Leveraged customer support tickets data from freshdesk to built a chatbot that answered user queries Integrated an RAG model with GPT to generate responses from both text and images, enhancing accuracy.',\n",
       " 'Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model.',\n",
       " 'Utilized the GPT model to generate clear & concise response comparing the suggested with prior selections.',\n",
       " 'PERSONAL PROJECTS May 2024 - July 2024 Enhancement of haptic alert systems for distracted pedestrians Engineered a smart wearable device that vibrates based on vehicle proximity, alerting pedestrians of traffic.',\n",
       " 'Detected vehicles using YOLO and calculated speed and employed edge computing with Raspberry Pi.',\n",
       " 'Utilized MQTT to communicate with Arduino for variable vibration intensity based on vehicle proximity. PDF',\n",
       " 'Answering AI Apr 2023 - May 2023 Streamlined QnA by using BERT embeddings to retrieve question relevant sentences from PDF documents.',\n",
       " 'Optimized response generation by assigning weights to sentences based on their alignment with the question.',\n",
       " 'Enhanced BERT model, fine-tuned on a dataset of QnA pairs; generating response with weighted relevance.',\n",
       " 'Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors.',\n",
       " 'Developed anomaly detection techniques to accurately identify irregular energy consumption patterns.',\n",
       " 'Recommended targeted corrective actions to mitigate consumption spikes & optimize consumption efficiency.',\n",
       " 'EDUCATION B.Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning.ai course for Deep Learning & its Optimization Analytics IBM, Probability & Statistics.',\n",
       " 'Extra curriculars: Head Research Analyst | Appetizer, Executive Member | STC']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PULKIT RAWAT UG (IV Year I Semester) Email: rpulkit610@gmail.com B.Tech.', 'PERSONAL PROJECTS May 2024 - July 2024 Enhancement of haptic alert systems for distracted pedestrians Engineered a smart wearable device that vibrates based on vehicle proximity, alerting pedestrians of traffic.', '(Civil Engineering) p_rawat@ce.iitr.ac.in Contact No: 6261360983 linkedin.com/in/pulkit-rawat/ AREAS OF INTEREST Data Analytics, Data Science, Natural Language Processing, Data Structure and Algorithm, Gymnasium WORK EXPERIENCE Data Scientist , BuildWealth Technologies Pvt Ltd Dec 2023 - Apr 2023 Leveraged customer support tickets data from freshdesk to built a chatbot that answered user queries Integrated an RAG model with GPT to generate responses from both text and images, enhancing accuracy.', 'Developed anomaly detection techniques to accurately identify irregular energy consumption patterns.', 'Extra curriculars: Head Research Analyst | Appetizer, Executive Member | STC', 'Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model.', 'Optimized response generation by assigning weights to sentences based on their alignment with the question.', 'Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors.', 'EDUCATION B.Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning.ai course for Deep Learning & its Optimization Analytics IBM, Probability & Statistics.', 'Utilized MQTT to communicate with Arduino for variable vibration intensity based on vehicle proximity. PDF']\n"
     ]
    }
   ],
   "source": [
    "text  = extract_text(\"D:\\Resumes\\off_campus.pdf\")\n",
    "resume_chunks = preProcess_chunk_text(text) \n",
    "question = \"what are my AREAS OF INTEREST\"\n",
    "top_relevant_chunks = get_most_relevant_sentences(question, resume_chunks, 10)\n",
    "print(top_relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a462f-3c50-46a8-aa2b-28e68bae290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
