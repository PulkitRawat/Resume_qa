{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "def preProcess_chunk_text(text:str, chunk_size = 500)->list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contain sentences, and number of words in each chunk does not exceed \n",
    "    specified chunk_size\n",
    "    Paramters:\n",
    "        text(str): parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size(int): Maximum number of words per chunk (Default: 500).\n",
    "    Returns:\n",
    "        list[list[str]]: list of chunks, where each chunk contains sentences having number of words is less than or equal to chunk_size.|\n",
    "        \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences= segment_into_sentences(text)\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    for sentence in sentences: \n",
    "        words = sentence.split()\n",
    "        word_count+= len(words)\n",
    "        if word_count>chunk_size:\n",
    "            word_count = len(words)\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "        chunk.append(sentence)\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text  = extract_text(\"C:\\\\Users\\\\HP\\\\OneDrive\\\\Pictures\\\\Screenshots\\\\Screenshot 2025-01-06 114334.png\")\n",
    "print(len(preProcess_chunk_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
