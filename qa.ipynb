{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320ff25b-7ce9-4b10-ae98-9f6cec720d43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromaDB\n",
      "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromaDB)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (2.10.6)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromaDB)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl.metadata (262 bytes)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.110.3)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromaDB)\n",
      "  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.15.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.31.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromaDB)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromaDB)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.52b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromaDB)\n",
      "  Downloading opentelemetry_sdk-1.31.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromaDB)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 30.7/67.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (1.71.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromaDB)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromaDB)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromaDB)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromaDB)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromaDB)\n",
      "  Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.9/42.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromaDB) (13.3.5)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromaDB) (23.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromaDB)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromaDB) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromaDB) (0.37.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromaDB) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromaDB) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.28.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (0.58.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromaDB) (1.26.18)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromaDB)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (4.25.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromaDB) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromaDB) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromaDB) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB) (1.63.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.31.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromaDB)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.52b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB)\n",
      "  Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromaDB) (3.8.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromaDB)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromaDB) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromaDB) (1.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromaDB) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromaDB) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromaDB) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromaDB) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromaDB) (0.27.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromaDB) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromaDB) (0.21.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading watchfiles-1.0.4-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromaDB)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (4.7.2)\n",
      "INFO: pip is looking at multiple versions of googleapis-common-protos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromaDB)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromaDB) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromaDB) (2023.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromaDB) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromaDB) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromaDB) (2.0.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromaDB) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromaDB) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromaDB) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromaDB) (0.4.8)\n",
      "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "   ---------------------------------------- 0.0/611.1 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 194.6/611.1 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 430.1/611.1 kB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 611.1/611.1 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.9/151.9 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.8/152.8 kB ? eta 0:00:00\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.7/2.0 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/2.0 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.31.0-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.9/55.9 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.52b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.52b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.4/183.4 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.31.0-py3-none-any.whl (118 kB)\n",
      "   ---------------------------------------- 0.0/118.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 118.9/118.9 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.6/133.6 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading posthog-3.21.0-py2.py3-none-any.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.6/79.6 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.3/62.3 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "   ---------------------------------------- 0.0/293.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 293.2/293.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 88.6/88.6 kB 5.2 MB/s eta 0:00:00\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 358.4/434.5 kB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.5/434.5 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading watchfiles-1.0.4-cp311-cp311-win_amd64.whl (284 kB)\n",
      "   ---------------------------------------- 0.0/284.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 284.2/284.2 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "   ---------------------------------------- 0.0/176.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 176.8/176.8 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53915 sha256=0d380d67cffa2e54ad8d067609440f3f30c7e1027cfbf13dad03bc932da03d10\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, websockets, tenacity, pyproject_hooks, protobuf, orjson, opentelemetry-util-http, oauthlib, mmh3, httptools, chroma-hnswlib, bcrypt, watchfiles, uvicorn, requests-oauthlib, posthog, opentelemetry-proto, googleapis-common-protos, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromaDB\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.6\n",
      "    Uninstalling protobuf-4.25.6:\n",
      "      Successfully uninstalled protobuf-4.25.6\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.63.0\n",
      "    Uninstalling googleapis-common-protos-1.63.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.63.0\n",
      "Successfully installed bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromaDB-0.6.3 durationpy-0.9 googleapis-common-protos-1.69.2 httptools-0.6.4 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 oauthlib-3.2.2 opentelemetry-exporter-otlp-proto-common-1.31.0 opentelemetry-exporter-otlp-proto-grpc-1.31.0 opentelemetry-instrumentation-0.52b0 opentelemetry-instrumentation-asgi-0.52b0 opentelemetry-instrumentation-fastapi-0.52b0 opentelemetry-proto-1.31.0 opentelemetry-sdk-1.31.0 opentelemetry-semantic-conventions-0.52b0 opentelemetry-util-http-0.52b0 orjson-3.10.15 posthog-3.21.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 tenacity-9.0.0 uvicorn-0.34.0 watchfiles-1.0.4 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\~crypt'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 2.17.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.33.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.4 which is incompatible.\n",
      "streamlit 1.33.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe54342-bae5-48e0-8412-99a474e40c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c8e797-6bb8-4163-a4a3-87a0f98888e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b1fd46-487a-47b7-9c79-0b6d08d864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "chroma_client = chromadb.PersistentClient(path = \"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name='resume_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79d5f5-f199-4b45-8aeb-b0975bb74760",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6a667c-5b0c-4297-9124-c2cb2e82d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path:str)->str:\n",
    "    \"\"\"\n",
    "    extract text from file(PDF or Image)\n",
    "    Parameters:\n",
    "        file_path(str): Path to file. Supported formats:  \"pdf\", \"jpg\", \"jpeg\", \"png\".\n",
    "    Returns:\n",
    "        str: extracted text form the file.\n",
    "    Raises:\n",
    "        ValueError: when the file type is unsupported\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    _,file_extension = os.path.splitext(file_path)\n",
    "    file_extension= file_extension.lower()\n",
    "    if file_extension in [\".pdf\"]:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text+= page.extract_text()\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing the pdf: {e}\"\n",
    "    elif file_extension in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        try:\n",
    "            image = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            return f\"Error in parsing in image: {e}\"\n",
    "    else:\n",
    "        raise ValueError(\"unsupported file type. Please use 'pdf', 'jpg', 'jpeg' or 'png'\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce8fd0-c238-4a57-8e2b-98bf085aa948",
   "metadata": {},
   "source": [
    "# Pre-Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910afec2-3cb3-4370-9625-4c5b2f46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    Cleans the extracted text including:\n",
    "     -Removing extra spaces and new Lines.\n",
    "     -Handling common ocr errors (e.g., 'ﬁ' to 'fi').\n",
    "     -Normalizing punctuation.\n",
    "    Parameters:\n",
    "        text(str): Text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # remove non-ASCII characters\n",
    "    text = re.sub(r'ﬁ', 'fi', text) # common ocr mistake\n",
    "    return text\n",
    "def segment_into_sentences(text: str)->list[str]:\n",
    "    \"\"\"\n",
    "    Segments cleaned text into individual sentences. the sentences are the stream of words in single line\n",
    "    Parameters:\n",
    "        text(str): Cleaned text to be segmented\n",
    "    Returns:\n",
    "        list[str]: list of sentences extracted from the text.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(\\.|\\n|\\t)')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences\n",
    "def preProcess_chunk_text(text: str, chunk_size=10) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Divide the sentences from the cleaned text into manageable chunks. Each chunk contains sentences, and the number of words is minimum the chunk_size.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Parsed text obtained from the file that is to be chunked.\n",
    "        chunk_size (int): Maximum number of words per chunk (Default: 20).\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: List of chunks, where each chunk contains sentences with a word count greater than or equal to chunk_size.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = segment_into_sentences(text)\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if(sentence == \".\"):\n",
    "            continue\n",
    "        chunk.append(sentence)\n",
    "        word_count += len(sentence.split())\n",
    "        if(word_count>chunk_size):\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = []\n",
    "            word_count = 0\n",
    "            \n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c610b-d04e-4a07-8d6b-31e7508b2f72",
   "metadata": {},
   "source": [
    "# Form Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ec7d82-2c7c-4194-ba0c-9def823b9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"BERT_FineTuned_Model2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8dcb2ac-13ce-4a22-b42f-55449db30c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text: str):\n",
    "    \"\"\"\n",
    "    extracts the key phrases  from the question.\n",
    "    Parametes:\n",
    "        text(str): text from which keywords will be extracted\n",
    "    Returns: \n",
    "        List[str]: a list of key words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ','VERB']] \n",
    "    return keywords\n",
    "    \n",
    "def get_sentence_embedding(text:str):\n",
    "    \"\"\"\n",
    "    Convert a sentence into its dense embedding usng RoBERTa.\n",
    "    Parameters:\n",
    "        text(str): the text to be converted into embedding.\n",
    "    Returns\n",
    "        embedding: the dense vector representing embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    embedding = hidden_states.mean(dim = 1).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e6629ca-b886-42fe-87d8-aa1c92f7bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(text_chunks:list[str]):\n",
    "    \"\"\"\n",
    "    stores text chunks and their embeddings into chroma DB.\n",
    "\n",
    "    Parameters:\n",
    "        text_chunks(list[str]): list of extracted text chunks.\n",
    "    Returns:\n",
    "        None(Stores the embedding in chromaDB)\n",
    "    \"\"\"\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        embedding = get_sentence_embedding(chunk).tolist()\n",
    "    \n",
    "        collection.add(\n",
    "            ids = [str(uuid.uuid4())],\n",
    "            embeddings = [embedding],\n",
    "            metadatas = [{'text':chunk}]\n",
    "        )\n",
    "    print(\"text_chunk was stored as embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1c7ec-ca0b-4824-835b-d2c6db1d662d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28f6b424-211b-4ac8-8071-c5a3a61314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentences(question: str, top_k = 7)->list[str]:\n",
    "    \"\"\"\n",
    "    find the most relevant sentences in the resume for a given question using chromaDB and boost the relevance using keyword matching\n",
    "\n",
    "    Parameters: \n",
    "        questions(str): the user's query.\n",
    "        resume_chunks(list): list of resume text chunks\n",
    "        top_k: maximum number of relvant sentences required(default 7)\n",
    "    Returns:\n",
    "        List[str]: the list of most relevant chunks\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(question)\n",
    "    question_embedding = get_sentence_embedding(question).numpy()\n",
    "    results = collection.query(\n",
    "        query_embeddings = [question_embedding],\n",
    "        n_results = int(top_k*5)\n",
    "    )\n",
    "\n",
    "    # print(\"results \\n\", results)\n",
    "    retrieved_chunks = results['metadatas'][0]\n",
    "    distances = results['distances'][0]\n",
    "    print(\"Retrieved Chunks:\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        print(f\"{i+1}. {chunk['text']} (Distance: {distances[i]})\")\n",
    "\n",
    "    text_chunks = [chunk['text'] for chunk in retrieved_chunks]\n",
    "    # print(text_chunks)\n",
    "    keyword_boost = []\n",
    "    for chunk,distance in zip(text_chunks, distances):\n",
    "        # print(chunk)\n",
    "        # print(distance)\n",
    "        chunk_keywords = extract_keywords(chunk)\n",
    "\n",
    "        match_count = len([chunk_keyword for chunk_keyword in chunk_keywords if chunk_keyword in keywords])\n",
    "        print(chunk)\n",
    "        print(chunk_keywords)\n",
    "        print(match_count)\n",
    "        \n",
    "        sim = 1/(1+distance)\n",
    "        boosted_score= sim + match_count*0.54\n",
    "        keyword_boost.append((boosted_score, chunk))\n",
    "        \n",
    "    keyword_boost.sort(key = lambda x:x[0], reverse = True)\n",
    "    # print(keyword_boost)\n",
    "    top_k_chunks = [chunk for _, chunk in keyword_boost[:top_k]]\n",
    "\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c9122-3699-4a79-95bc-37965de3e773",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc10b78f-a4e9-4558-9073-5e103f30a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qa_model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_tokenizer = BertTokenizer.from_pretrained(qa_model_path)\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(qa_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1ec3432-8154-481c-9cdb-d44d44b1d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, relevant_sentences:list[str])->str:\n",
    "    \"\"\"\n",
    "    Generate a clear and concise answer to the question based on the most relevant sentences.\n",
    "\n",
    "    Parameters:\n",
    "        question(str):The user's query.\n",
    "        relevant_sentences(list[str]): list of most relevant sentences.\n",
    "        \n",
    "    Returns:\n",
    "        str: Generate answer.\n",
    "    \"\"\"\n",
    "\n",
    "    context = \" \".join(relevant_sentences)\n",
    "    inputs = qa_tokenizer.encode_plus(question, context, return_tensors='pt', truncation = True, max_length = 512)\n",
    "    # print(inputs)\n",
    "    with torch.no_grad():\n",
    "        output = qa_model(**inputs)\n",
    "        start_scores,end_scores, = output.start_logits, output.end_logits\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index]))\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db028614-d2fd-4f77-b787-e78346cdfa24",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2a7c88c-3df2-4890-8202-bc6768b546ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skills']\n"
     ]
    }
   ],
   "source": [
    "print(extract_keywords(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9630b76a-fcd6-4b04-bfc5-13bf17f696c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collection deleted and recreated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 35 is greater than number of elements in index 17, updating n_results = 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_chunk was stored as embedding\n",
      "Retrieved Chunks:\n",
      "1. Utilized the GPT model to generate clear & concise response comparing the suggested with prior selections (Distance: 224.90476619830622)\n",
      "2. Optimized response generation by assigning weights to sentences based on their alignment with the question (Distance: 226.94681803478935)\n",
      "3. Recommended targeted corrective actions to mitigate consumption spikes & optimize consumption efficiency (Distance: 238.03595262031405)\n",
      "4. Developed anomaly detection techniques to accurately identify irregular energy consumption patterns (Distance: 281.3277128566909)\n",
      "5. Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors (Distance: 284.5439654890591)\n",
      "6. Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model (Distance: 290.4553420128084)\n",
      "7. PERSONAL PROJECTS May 2024 - July 2024 Enhancement of haptic alert systems for distracted pedestrians Engineered a smart wearable device that vibrates based on vehicle proximity, alerting pedestrians of traffic (Distance: 294.19760143787124)\n",
      "8. Enhanced BERT model, fine-tuned on a dataset of QnA pairs; generating response with weighted relevance (Distance: 309.07239815021325)\n",
      "9. PDF Answering AI Apr 2023 - May 2023 Streamlined QnA by using BERT embeddings to retrieve question relevant sentences from PDF documents (Distance: 320.3832381767944)\n",
      "10. com/in/pulkit-rawat/ AREAS OF INTEREST Data Analytics, Data Science, Natural Language Processing, Data Structure and Algorithm, Gymnasium WORK EXPERIENCE Data Scientist , BuildWealth Technologies Pvt Ltd Dec 2023 - Apr 2023 Leveraged customer support tickets data from freshdesk to built a chatbot that answered user queries Integrated an RAG model with GPT to generate responses from both text and images, enhancing accuracy (Distance: 336.9880575000417)\n",
      "11. Utilized MQTT to communicate with Arduino for variable vibration intensity based on vehicle proximity (Distance: 342.72221999150975)\n",
      "12. EDUCATION B Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning (Distance: 347.50430672374137)\n",
      "13. Detected vehicles using YOLO and calculated speed and employed edge computing with Raspberry Pi (Distance: 355.8746308469337)\n",
      "14. Extra curriculars: Head Research Analyst | Appetizer, Executive Member | STC (Distance: 405.0859063414205)\n",
      "15. PULKIT RAWAT UG (IV Year I Semester) Email: rpulkit610@gmail com B (Distance: 411.4140671749741)\n",
      "16. ai course for Deep Learning & its Optimization Analytics IBM, Probability & Statistics (Distance: 443.5327033913548)\n",
      "17. Tech (Civil Engineering) p_rawat@ce iitr ac in Contact No: 6261360983 linkedin (Distance: 503.9324585624375)\n",
      "Utilized the GPT model to generate clear & concise response comparing the suggested with prior selections\n",
      "['utilized', 'gpt', 'model', 'generate', 'clear', 'concise', 'response', 'comparing', 'suggested', 'prior', 'selections']\n",
      "0\n",
      "Optimized response generation by assigning weights to sentences based on their alignment with the question\n",
      "['optimized', 'response', 'generation', 'assigning', 'weights', 'sentences', 'based', 'alignment', 'question']\n",
      "0\n",
      "Recommended targeted corrective actions to mitigate consumption spikes & optimize consumption efficiency\n",
      "['recommended', 'targeted', 'corrective', 'actions', 'mitigate', 'consumption', 'spikes', 'optimize', 'consumption', 'efficiency']\n",
      "0\n",
      "Developed anomaly detection techniques to accurately identify irregular energy consumption patterns\n",
      "['developed', 'anomaly', 'detection', 'techniques', 'identify', 'irregular', 'energy', 'consumption', 'patterns']\n",
      "0\n",
      "Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors\n",
      "['developed', 'user', 'friendly', 'interface', 'bootstrap', 'flask', 'enabling', 'seamless', 'user', 'interaction', 'system', 'predictive', 'optimization', 'anomaly', 'detection', 'energy', 'consumption', 'apr', 'may', 'used', 'facebook', 'prophet', 'forecast', 'energy', 'consumption', 'trends', 'analyzing', 'weather', 'household', 'factors']\n",
      "0\n",
      "Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model\n",
      "['analyzed', 'fund', 'merits', 'user', 'purchase', 'patterns', 'risk', 'profiles', 'develop', 'robust', 'fund', 'suggestion', 'model']\n",
      "0\n",
      "PERSONAL PROJECTS May 2024 - July 2024 Enhancement of haptic alert systems for distracted pedestrians Engineered a smart wearable device that vibrates based on vehicle proximity, alerting pedestrians of traffic\n",
      "['personal', 'projects', 'may', 'july', 'enhancement', 'haptic', 'alert', 'systems', 'distracted', 'pedestrians', 'engineered', 'smart', 'wearable', 'device', 'vibrates', 'based', 'vehicle', 'proximity', 'alerting', 'pedestrians', 'traffic']\n",
      "0\n",
      "Enhanced BERT model, fine-tuned on a dataset of QnA pairs; generating response with weighted relevance\n",
      "['enhanced', 'bert', 'model', 'tuned', 'dataset', 'qna', 'pairs', 'generating', 'response', 'weighted', 'relevance']\n",
      "0\n",
      "PDF Answering AI Apr 2023 - May 2023 Streamlined QnA by using BERT embeddings to retrieve question relevant sentences from PDF documents\n",
      "['pdf', 'answering', 'ai', 'apr', 'may', 'streamlined', 'qna', 'using', 'bert', 'embeddings', 'retrieve', 'question', 'relevant', 'sentences', 'pdf', 'documents']\n",
      "0\n",
      "com/in/pulkit-rawat/ AREAS OF INTEREST Data Analytics, Data Science, Natural Language Processing, Data Structure and Algorithm, Gymnasium WORK EXPERIENCE Data Scientist , BuildWealth Technologies Pvt Ltd Dec 2023 - Apr 2023 Leveraged customer support tickets data from freshdesk to built a chatbot that answered user queries Integrated an RAG model with GPT to generate responses from both text and images, enhancing accuracy\n",
      "['com', 'pulkit', 'areas', 'interest', 'data', 'analytics', 'data', 'science', 'natural', 'language', 'processing', 'data', 'structure', 'algorithm', 'gymnasium', 'work', 'experience', 'data', 'scientist', 'buildwealth', 'technologies', 'pvt', 'ltd', 'dec', 'apr', 'leveraged', 'customer', 'support', 'tickets', 'data', 'freshdesk', 'built', 'chatbot', 'answered', 'user', 'queries', 'integrated', 'rag', 'model', 'gpt', 'generate', 'responses', 'text', 'images', 'enhancing', 'accuracy']\n",
      "0\n",
      "Utilized MQTT to communicate with Arduino for variable vibration intensity based on vehicle proximity\n",
      "['utilized', 'mqtt', 'communicate', 'arduino', 'variable', 'vibration', 'intensity', 'based', 'vehicle', 'proximity']\n",
      "0\n",
      "EDUCATION B Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning\n",
      "['education', 'b', 'tech', 'civil', 'engineering', 'present', 'indian', 'institute', 'technology', 'roorkee', 'additional', 'information', 'computer', 'languages', 'python', 'c++', 'sql', 'soft', 'skills', 'problem', 'solving', 'leadership', 'collaborative', 'additional', 'courses', 'deeplearning']\n",
      "1\n",
      "Detected vehicles using YOLO and calculated speed and employed edge computing with Raspberry Pi\n",
      "['detected', 'vehicles', 'using', 'calculated', 'speed', 'employed', 'edge', 'computing', 'raspberry', 'pi']\n",
      "0\n",
      "Extra curriculars: Head Research Analyst | Appetizer, Executive Member | STC\n",
      "['extra', 'curriculars', 'head', 'research', 'analyst', '|', 'appetizer', 'executive', 'member', '|', 'stc']\n",
      "0\n",
      "PULKIT RAWAT UG (IV Year I Semester) Email: rpulkit610@gmail com B\n",
      "['pulkit', 'rawat', 'ug', 'iv', 'year', 'i', 'semester', 'email', 'rpulkit610@gmail', 'com', 'b']\n",
      "0\n",
      "ai course for Deep Learning & its Optimization Analytics IBM, Probability & Statistics\n",
      "['ai', 'course', 'deep', 'learning', 'optimization', 'analytics', 'ibm', 'probability', 'statistics']\n",
      "0\n",
      "Tech (Civil Engineering) p_rawat@ce iitr ac in Contact No: 6261360983 linkedin\n",
      "['tech', 'civil', 'engineering', 'p_rawat@ce', 'iitr', 'ac', 'contact', 'no', 'linkedin']\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'problem solving , leadership , collaborative'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  = extract_text(\"D:\\Resumes\\off_campus.pdf\")\n",
    "resume_chunks = preProcess_chunk_text(text) \n",
    "chroma_client.delete_collection(name=\"resume_embeddings\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"resume_embeddings\")  # Recreate it\n",
    "print(\"✅ Collection deleted and recreated!\")\n",
    "store_embeddings(resume_chunks)\n",
    "question = \"what are the skills?\"\n",
    "top_relevant_sentences = get_most_relevant_sentences(question)\n",
    "answer = generate_answer(question, top_relevant_sentences)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "05c4a976-2149-4daa-9609-ba45a39554bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDUCATION B Tech in Civil Engineering 2021 - present Indian Institute Technology Roorkee ADDITIONAL INFORMATION Computer Languages: Python, C++, SQL Soft skills: Problem Solving, Leadership, Collaborative Additional Courses: DeepLearning',\n",
       " 'Utilized the GPT model to generate clear & concise response comparing the suggested with prior selections',\n",
       " 'Optimized response generation by assigning weights to sentences based on their alignment with the question',\n",
       " 'Recommended targeted corrective actions to mitigate consumption spikes & optimize consumption efficiency',\n",
       " 'Developed anomaly detection techniques to accurately identify irregular energy consumption patterns',\n",
       " 'Developed a user-friendly interface with Bootstrap and Flask, enabling seamless user interaction with system Predictive Optimization and Anomaly Detection in Energy Consumption Apr 2024 - May 2024 Used Facebook Prophet to forecast energy consumption trends by analyzing weather and household factors',\n",
       " 'Analyzed fund merits, user purchase patterns and risk profiles to develop a robust fund suggestion model']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_relevant_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d47b3a-2acc-425a-a9e3-f66d25bba959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
